{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestr√≠a en Inteligencia Artificial Aplicada\n",
        "#### Tecnol√≥gico de Monterrey\n",
        "#### Prof Luis Eduardo Falc√≥n Morales\n",
        "\n",
        "### **Actividad en Equipos - Semana 10 : LLM y IA en tu lugar de trabajo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matr√≠culas:**\n",
        "\n",
        "  *   Carlos Pano Hernandez - A01066264\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "\n",
        "* **N√∫mero de Equipo: 64**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Parte 1: Carga de credenciales OpenAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from the specific .env file\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv('.env')\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Prueba de la API de OpenAI\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Hola, como estas? (Responde en 1 palabra y en Aleman)\"\n",
        ")\n",
        "\n",
        "# Imprimir la respuesta como test\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYgtCvvJSmq"
      },
      "source": [
        "# **Ejercicio 2a:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQjVP2HkoZY"
      },
      "source": [
        "* #### **Comenten el por qu√© del modelo seleccionado para extracci√≥n del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $‚Üí$ {audio01:fabula01, ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3k5sLGhnO1d"
      },
      "outputs": [],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"\"\"Hola! Stori es una Fintech que se dedica a la creaci√≥n de productos financieros (tarjetas de cr√©dito, cuentas de ahorro con dinero creciente, pr√©stamos, etc.).\n",
        "    En Github, tenemos una organizaci√≥n donde se encuentran TODOS los repositorios que contienen el c√≥digo de Stori. Por lo que para este proyecto, la fuente ser√° Github. Los repos de Github, son privados (token: api_key = os.getenv('CPANO_GITHUB_TOKEN'))\n",
        "    \n",
        "    Realiza el plantamiento de un nuevo proyecto de IA para Stori que resuelva el siguiente problema:\n",
        "    - Problema: Cada repo, ejemplo (https://github.com/credifranco/deposits-transactions-securedcard/blob/dev/TROUBLESHOOTING_RUNBOOK.md), tiene un runbook que es un documento que contiene documentaci√≥n relevante para dar soporte a ingenieros que no tengan contexto de como resolver un problema.\n",
        "    - Soluci√≥n: Realiza un pipeline de IA que lea el runbook y genere un resumen de lo que se menciona en el runbook. El runbook debe ser descargado de Github, le√≠do e interpretado por un modelo de LLM para ser inyecto como un RAG. Usando Gradio, despliega una interfaz donde se puede ingresar una pregunta y el modelo de LLM genere una respuesta basada en el RAG de los runbooks. Pueden ser varios, entonces se debe dise√±ar un Pipeline de IA que lea los runbooks y genere un RAG para cada uno de ellos.\n",
        "    - Resultados: Una interfaz que pueda resolver dudas en tiempo real de los ingenieros de Stori.\n",
        "    \n",
        "    Retorna el plantamiento de este proyecto, considerando las herramientas disponibles, RAG, LLM, etc. As√≠ como el plan de acci√≥n para llevarlo a cabo.\n",
        "    \n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Imprimir la respuesta como test\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --user PyGithub TextLoader langchain_community langchain_openai chromadb gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stori_runbook_chatbot_poc.ipynb\n",
        "\n",
        "# --- Importar Librer√≠as ---\n",
        "import os\n",
        "import shutil\n",
        "import logging # Importa el m√≥dulo logging\n",
        "from github import Github, UnknownObjectException\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from IPython.display import Markdown, display # Solo si est√°s ejecutando en Jupyter/IPython\n",
        "import gradio as gr\n",
        "\n",
        "# Cargar variables de entorno\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('.env')\n",
        "\n",
        "# --- Configuraci√≥n de Logging ---\n",
        "# Configura el nivel de logging a INFO para ver los mensajes de depuraci√≥n.\n",
        "# Puedes cambiar a logging.DEBUG para mensajes m√°s detallados si los a√±ades.\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- 0. Configuraci√≥n de Variables de Entorno y API Keys ---\n",
        "logging.info(\"--- 0. Configuraci√≥n de Variables de Entorno y API Keys ---\")\n",
        "GITHUB_TOKEN = os.getenv('CPANO_GITHUB_TOKEN')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not GITHUB_TOKEN:\n",
        "    logging.error(\"Error: CPANO_GITHUB_TOKEN no est√° configurado. Por favor, config√∫ralo en tus variables de entorno o en un archivo .env.\")\n",
        "    raise ValueError(\"Error: CPANO_GITHUB_TOKEN no est√° configurado. Por favor, config√∫ralo en tus variables de entorno o en un archivo .env.\")\n",
        "if not OPENAI_API_KEY:\n",
        "    logging.error(\"Error: OPENAI_API_KEY no est√° configurado. Por favor, config√∫ralo en tus variables de entorno o en un archivo .env.\")\n",
        "    raise ValueError(\"Error: OPENAI_API_KEY no est√° configurado. Por favor, config√∫ralo en tus variables de entorno o en un archivo .env.\")\n",
        "logging.info(\"Variables de entorno GITHUB_TOKEN y OPENAI_API_KEY cargadas.\")\n",
        "\n",
        "# --- Parte 1: Descarga del Runbook ---\n",
        "logging.info(\"--- Parte 1: Descarga del Runbook de GitHub ---\")\n",
        "\n",
        "ORG_NAME = \"credifranco\"\n",
        "REPO_NAME = \"deposits-transactions-securedcard\"\n",
        "RUNBOOK_PATH = \"TROUBLESHOOTING_RUNBOOK.md\"\n",
        "\n",
        "LOCAL_RUNBOOK_DIR = \"./downloaded_runbooks\"\n",
        "LOCAL_RUNBOOK_FILE = os.path.join(LOCAL_RUNBOOK_DIR, f\"{REPO_NAME}_{RUNBOOK_PATH.replace('/', '_')}\")\n",
        "\n",
        "os.makedirs(LOCAL_RUNBOOK_DIR, exist_ok=True)\n",
        "download_successful = False\n",
        "logging.info(f\"Directorio local para runbooks: {LOCAL_RUNBOOK_DIR}\")\n",
        "\n",
        "try:\n",
        "    g = Github(GITHUB_TOKEN)\n",
        "    org = g.get_organization(ORG_NAME)\n",
        "    repo = org.get_repo(REPO_NAME)\n",
        "    logging.info(f\"Conectado a GitHub. Org: {ORG_NAME}, Repo: {REPO_NAME}\")\n",
        "    \n",
        "    file_content = None\n",
        "    try:\n",
        "        logging.info(f\"Buscando el runbook en la rama 'dev'...\")\n",
        "        file_content = repo.get_contents(RUNBOOK_PATH, ref=\"dev\")\n",
        "        logging.info(f\"‚úÖ Runbook encontrado en la rama 'dev'.\")\n",
        "    except UnknownObjectException:\n",
        "        logging.warning(f\"‚ùå Runbook no encontrado en la rama 'dev'.\")\n",
        "        logging.info(f\"Buscando en la rama por defecto del repositorio...\")\n",
        "        try:\n",
        "            file_content = repo.get_contents(RUNBOOK_PATH)\n",
        "            logging.info(f\"‚úÖ Runbook encontrado en la rama por defecto.\")\n",
        "        except UnknownObjectException:\n",
        "            logging.error(f\"‚ùå Runbook '{RUNBOOK_PATH}' NO encontrado en el repositorio '{ORG_NAME}/{REPO_NAME}' en ninguna de las ramas principales ('dev' o por defecto).\")\n",
        "            logging.error(\"Por favor, verifica la ruta del archivo y los permisos de tu token.\")\n",
        "    \n",
        "    if file_content:\n",
        "        decoded_content = file_content.decoded_content.decode('utf-8')\n",
        "        with open(LOCAL_RUNBOOK_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(decoded_content)\n",
        "        logging.info(f\"--- üéâ √âXITO EN LA DESCARGA üéâ ---\")\n",
        "        logging.info(f\"El runbook '{RUNBOOK_PATH}' se ha descargado exitosamente a: '{LOCAL_RUNBOOK_FILE}'\")\n",
        "        download_successful = True\n",
        "    else:\n",
        "        logging.warning(\"\\n--- ‚ö†Ô∏è FALLO EN LA DESCARGA ‚ö†Ô∏è ---\")\n",
        "        logging.warning(\"No se pudo descargar el runbook. Verifica los mensajes anteriores.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.critical(f\"--- ‚ùå ERROR CR√çTICO DURANTE LA DESCARGA ‚ùå ---\")\n",
        "    logging.critical(f\"Ocurri√≥ un error inesperado: {e}\", exc_info=True) # exc_info=True para el traceback completo\n",
        "    logging.critical(\"Aseg√∫rate de que tu CPANO_GITHUB_TOKEN sea v√°lido y tenga los permisos necesarios (repo) para acceder a la organizaci√≥n y el repositorio especificados.\")\n",
        "\n",
        "finally:\n",
        "    if not download_successful:\n",
        "        if os.path.exists(LOCAL_RUNBOOK_DIR) and not os.listdir(LOCAL_RUNBOOK_DIR):\n",
        "            os.rmdir(LOCAL_RUNBOOK_DIR)\n",
        "            logging.info(f\"Directorio vac√≠o '{LOCAL_RUNBOOK_DIR}' eliminado.\")\n",
        "        logging.info(\"--- No se puede continuar con la Parte 2 sin un runbook descargado. ---\")\n",
        "        exit() \n",
        "\n",
        "logging.info(\"\\n--- Fin de la Parte 1 ---\\n\")\n",
        "\n",
        "# --- Parte 2: Procesamiento del Runbook y Configuraci√≥n del RAG ---\n",
        "logging.info(\"--- Parte 2: Procesamiento del Runbook y Configuraci√≥n del RAG ---\")\n",
        "\n",
        "# 2.1 Carga del Documento\n",
        "logging.info(\"2.1 Cargando el runbook descargado...\")\n",
        "loader = TextLoader(LOCAL_RUNBOOK_FILE, encoding=\"utf-8\")\n",
        "documents = loader.load()\n",
        "logging.info(f\"Runbook cargado. Contiene {len(documents)} documentos (esperado: 1). Longitud del contenido: {len(documents[0].page_content)} caracteres.\")\n",
        "\n",
        "# 2.2 Divisi√≥n del Documento en Chunks\n",
        "logging.info(\"2.2 Dividiendo el documento en chunks...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "logging.info(f\"Documento dividido en {len(texts)} chunks.\")\n",
        "# logging.debug(f\"Primer chunk: {texts[0].page_content[:200]}...\") # Para depuraci√≥n muy detallada\n",
        "\n",
        "# 2.3 Generaci√≥n de Embeddings y Almacenamiento en ChromaDB\n",
        "logging.info(\"2.3 Generando embeddings y almacenando en ChromaDB...\")\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
        "persist_directory = \"./chroma_db\"\n",
        "\n",
        "# Limpiar ChromaDB si ya existe para asegurar una carga fresca\n",
        "if os.path.exists(persist_directory):\n",
        "    logging.info(f\"Eliminando directorio existente de ChromaDB: {persist_directory}\")\n",
        "    shutil.rmtree(persist_directory)\n",
        "    \n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embeddings_model,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "vectordb.persist()\n",
        "logging.info(f\"Embeddings generados y almacenados en ChromaDB en '{persist_directory}'.\")\n",
        "\n",
        "# 2.4 Configuraci√≥n del LLM y la Cadena de RAG\n",
        "logging.info(\"2.4 Configurando el LLM y la cadena de RAG...\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "logging.info(f\"LLM configurado: {llm.model_name}, temperatura: {llm.temperature}\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "logging.info(\"LLM y cadena de RAG configurados exitosamente.\")\n",
        "\n",
        "logging.info(\"\\n--- Fin de la Parte 2 ---\\n\")\n",
        "\n",
        "# --- Parte 3: Interfaz de Chatbot con Gradio ---\n",
        "# ... (all your imports and setup from above) ...\n",
        "\n",
        "logging.info(\"--- Parte 3: Iniciando Interfaz de Chatbot con Gradio ---\")\n",
        "\n",
        "def chat_with_runbook(question, history):\n",
        "    logging.info(f\"Pregunta recibida del chatbot: '{question}'\")\n",
        "\n",
        "    if not question.strip():\n",
        "        logging.warning(\"Pregunta vac√≠a recibida.\")\n",
        "        # Return an error message and the current history *unchanged* if the input is invalid\n",
        "        return \"Por favor, ingresa una pregunta v√°lida.\", history\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Invocando qa_chain con query: '{question}'\")\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "        logging.info(f\"Resultado de qa_chain.invoke: {result}\")\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            raise TypeError(f\"qa_chain.invoke() debe retornar un diccionario, pero retorn√≥: {type(result)}\")\n",
        "        if \"result\" not in result:\n",
        "            raise KeyError(\"La clave 'result' no se encontr√≥ en el diccionario de respuesta de qa_chain.\")\n",
        "        if \"source_documents\" not in result:\n",
        "            raise KeyError(\"La clave 'source_documents' no se encontr√≥ en el diccionario de respuesta de qa_chain.\")\n",
        "\n",
        "        answer = result[\"result\"]\n",
        "        source_docs = result[\"source_documents\"]\n",
        "\n",
        "        logging.info(f\"Respuesta generada (primeros 100 chars): {answer[:100]}...\")\n",
        "        logging.info(f\"N√∫mero de documentos fuente recuperados: {len(source_docs)}\")\n",
        "\n",
        "        sources_info = \"\\n\\n**Fuentes (Extractos):**\\n\"\n",
        "        if source_docs:\n",
        "            for i, doc in enumerate(source_docs):\n",
        "                logging.info(f\"Procesando documento fuente {i+1}. Tipo: {type(doc)}\")\n",
        "                if not hasattr(doc, 'page_content') or not isinstance(doc.page_content, str):\n",
        "                    logging.error(f\"Documento fuente {i+1} no tiene page_content v√°lido. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                page_content_preview = doc.page_content[:250].replace('\\n', ' ')\n",
        "                if len(doc.page_content) > 250:\n",
        "                    page_content_preview += '...'\n",
        "                sources_info += f\"- **Chunk {i+1}:** {page_content_preview}\\n\"\n",
        "                if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
        "                    sources_info += f\" (Fuente: {doc.metadata['source']})\\n\"\n",
        "                logging.debug(f\"Chunk {i+1} a√±adido a fuentes: {page_content_preview}\")\n",
        "        else:\n",
        "            sources_info = \"\\n\\n*No se encontraron fuentes directas en los runbooks para esta consulta.*\"\n",
        "            logging.info(\"No se recuperaron documentos fuente para esta consulta.\")\n",
        "\n",
        "        full_response = f\"{answer}{sources_info}\"\n",
        "        logging.info(f\"Respuesta completa para Gradio (primeros 200 chars): {full_response[:200]}...\")\n",
        "\n",
        "        # IMPORTANT: Append the current turn (question, full_response) to the history\n",
        "        # Gradio expects the *updated* history back.\n",
        "        # Ensure 'history' is a list and append a list/tuple for the current turn.\n",
        "        # If history is None or not initialized, ensure it starts as an empty list.\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append([question, full_response]) # Append the user's question and the bot's response\n",
        "\n",
        "        return \"\", history # Return an empty string for the immediate output, and the updated history\n",
        "        # Gradio's ChatInterface will then render the history.\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Ocurri√≥ un error al procesar tu pregunta: {e}\"\n",
        "        logging.error(f\"Error durante el procesamiento de la pregunta: {e}\", exc_info=True)\n",
        "        # For errors, you still need to return a message and the history.\n",
        "        # Here, we append the user's question and the error message to history.\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append([question, error_message])\n",
        "        return \"\", history # Return an empty string for the output, and the updated history with the error.\n",
        "\n",
        "# Crea la interfaz de Gradio para el chatbot\n",
        "chatbot_interface = gr.ChatInterface(\n",
        "    fn=chat_with_runbook,\n",
        "    chatbot=gr.Chatbot(height=500),\n",
        "    textbox=gr.Textbox(placeholder=\"Haz tu pregunta sobre el runbook...\", container=False, scale=7),\n",
        "    title=\"ü§ñ Asistente de Soporte de Stori (PoC RAG con Chatbot)\",\n",
        "    description=\"Pregunta sobre la documentaci√≥n interna de los runbooks de troubleshooting de Stori. Las respuestas son generadas por un LLM basado en la informaci√≥n recuperada de tus documentos.\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# Lanza la interfaz de Gradio\n",
        "logging.info(\"Gradio iniciado. Accede a la interfaz a trav√©s del enlace que aparecer√° a continuaci√≥n.\")\n",
        "chatbot_interface.launch(inbrowser=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1BtP-Sk0DT-M",
        "6uYgtCvvJSmq",
        "NM0D83j8EWiN",
        "6PKaB_Ge0Shc",
        "i2ywrmsMP_EF",
        "Blrrs1sWwkSx",
        "Kx-dZSFJz9cK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "master-ai-tec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
