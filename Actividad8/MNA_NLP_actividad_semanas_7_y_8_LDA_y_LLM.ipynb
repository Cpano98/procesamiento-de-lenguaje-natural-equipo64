{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Actividad en Equipos - Semana 10 : LLM y IA en tu lugar de trabajo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matrículas:**\n",
        "\n",
        "  *   Carlos Pano Hernandez - A01066264\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "\n",
        "* **Número de Equipo: 64**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Parte 1: Carga de credenciales OpenAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gut.\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from the specific .env file\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv('.env')\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Prueba de la API de OpenAI\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Hola, como estas? (Responde en 1 palabra y en Aleman)\"\n",
        ")\n",
        "\n",
        "# Imprimir la respuesta como test\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYgtCvvJSmq"
      },
      "source": [
        "# **Ejercicio 2a:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQjVP2HkoZY"
      },
      "source": [
        "* #### **Comenten el por qué del modelo seleccionado para extracción del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $→$ {audio01:fabula01, ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C3k5sLGhnO1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Plantamiento del Proyecto de IA para Stori: Generación de Resúmenes y Respuestas de Runbooks\n",
            "\n",
            "### Objetivo del Proyecto\n",
            "Desarrollar un pipeline de IA que lea runbooks existentes en repositorios de GitHub y genere un sistema de Respuesta Adaptativa Generativa (RAG) que permita a los ingenieros de Stori hacer preguntas y recibir respuestas basadas en la documentación de soporte.\n",
            "\n",
            "### Herramientas y Tecnologías\n",
            "1. **API de GitHub**: Para acceder y descargar runbooks desde repositorios privados.\n",
            "2. **Python**: Lenguaje de programación principal.\n",
            "3. **Librerías de IA**:\n",
            "   - **Transformers (Hugging Face)**: Para utilizar modelos de Lenguaje de Aprendizaje (LLM).\n",
            "   - **LangChain**: Para construir el pipeline de RAG.\n",
            "4. **Gradio**: Para crear una interfaz web interactiva.\n",
            "5. **Pandas y NumPy**: Para manipulación de datos si es necesario.\n",
            "6. **dotenv**: Para gestionar variables de entorno.\n",
            "7. **vectores Python para RAG**: Para almacenar y buscar información más eficientemente.\n",
            "\n",
            "### Plan de Acción\n",
            "\n",
            "#### Fase 1: Recolección de Datos\n",
            "\n",
            "1. **Configuración del Entorno**:\n",
            "   - Configurar el acceso a la API de GitHub usando el token.\n",
            "\n",
            "2. **Desarrollo de un Script de Descarga**:\n",
            "   - Crear un script en Python que utilice la API de GitHub para:\n",
            "     - Listar todos los repositorios en la organización de Stori.\n",
            "     - Buscar y descargar los archivos runbook (por ejemplo, archivos `.md`) de cada repositorio.\n",
            "\n",
            "#### Fase 2: Procesamiento de Documentos\n",
            "\n",
            "3. **Lectura y Preprocesamiento**:\n",
            "   - Leer los archivos descargados y preprocesar el texto para eliminar caracteres innecesarios, encabezados, etc.\n",
            "\n",
            "4. **Generación de Resúmenes**:\n",
            "   - Implementar un modelo de LLM para resumir el contenido de cada runbook.\n",
            "   - Almacenar resúmenes generados junto con el texto original para referencias futuras.\n",
            "\n",
            "#### Fase 3: Construcción del RAG\n",
            "\n",
            "5. **Construir el Pipeline RAG**:\n",
            "   - Integrar la funcionalidad de búsqueda y respuesta a preguntas utilizando un modelo LLM con el resumen generado.\n",
            "   - Utilizar vectores para indexar y recuperar información de manera eficiente.\n",
            "\n",
            "#### Fase 4: Creación de la Interfaz\n",
            "\n",
            "6. **Desarrollo de la Interfaz en Gradio**:\n",
            "   - Crear una interfaz de usuario sencilla en Gradio donde los ingenieros puedan ingresar preguntas.\n",
            "   - Mostrar la respuesta generada a partir del modelo LLM basado en el RAG.\n",
            "\n",
            "#### Fase 5: Pruebas y Validación\n",
            "\n",
            "7. **Testing del Sistema**:\n",
            "   - Realizar pruebas del sistema usando ejemplos de preguntas típicas que podrían tener los ingenieros de Stori.\n",
            "   - Ajustar el modelo LLM y el pipeline de RAG según sea necesario.\n",
            "\n",
            "8. **Validación con Usuarios**:\n",
            "   - Implementar una fase de feedback con un grupo de ingenieros para recoger sus opiniones sobre la eficacia de la interfaz y las respuestas generadas.\n",
            "\n",
            "#### Fase 6: Despliegue y Mantenimiento\n",
            "\n",
            "9. **Despliegue**:\n",
            "   - Poner el sistema en producción en un servidor accesible para todos los ingenieros de Stori.\n",
            "\n",
            "10. **Mantenimiento**:\n",
            "   - Establecer un plan de mantenimiento para actualizar el sistema con nuevos runbooks y mejorar el modelo con base en el feedback de los usuarios.\n",
            "\n",
            "### Resultados Esperados\n",
            "- Implementación de un sistema que permita a los ingenieros de Stori hacer preguntas y recibir respuestas basadas en runbooks de manera dinámica.\n",
            "- Mejora en la eficiencia de resolución de problemas y apoyo al personal técnico en el manejo de documentación.\n",
            "\n",
            "### Conclusión\n",
            "Este planteamiento crea un pipeline de IA robusto y útil que no solo mejora el acceso a la documentación de soporte, sino que también permite la interacción en tiempo real, haciendo más eficiente el trabajo de los ingenieros en Stori.\n"
          ]
        }
      ],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"\"\"Hola! Stori es una Fintech que se dedica a la creación de productos financieros (tarjetas de crédito, cuentas de ahorro con dinero creciente, préstamos, etc.).\n",
        "    En Github, tenemos una organización donde se encuentran TODOS los repositorios que contienen el código de Stori. Por lo que para este proyecto, la fuente será Github. Los repos de Github, son privados (token: api_key = os.getenv('CPANO_GITHUB_TOKEN'))\n",
        "    \n",
        "    Realiza el plantamiento de un nuevo proyecto de IA para Stori que resuelva el siguiente problema:\n",
        "    - Problema: Cada repo, ejemplo (https://github.com/credifranco/deposits-transactions-securedcard/blob/dev/TROUBLESHOOTING_RUNBOOK.md), tiene un runbook que es un documento que contiene documentación relevante para dar soporte a ingenieros que no tengan contexto de como resolver un problema.\n",
        "    - Solución: Realiza un pipeline de IA que lea el runbook y genere un resumen de lo que se menciona en el runbook. El runbook debe ser descargado de Github, leído e interpretado por un modelo de LLM para ser inyecto como un RAG. Usando Gradio, despliega una interfaz donde se puede ingresar una pregunta y el modelo de LLM genere una respuesta basada en el RAG de los runbooks. Pueden ser varios, entonces se debe diseñar un Pipeline de IA que lea los runbooks y genere un RAG para cada uno de ellos.\n",
        "    - Resultados: Una interfaz que pueda resolver dudas en tiempo real de los ingenieros de Stori.\n",
        "    \n",
        "    Retorna el plantamiento de este proyecto, considerando las herramientas disponibles, RAG, LLM, etc. Así como el plan de acción para llevarlo a cabo.\n",
        "    \n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Imprimir la respuesta como test\n",
        "print(response.output_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1BtP-Sk0DT-M",
        "6uYgtCvvJSmq",
        "NM0D83j8EWiN",
        "6PKaB_Ge0Shc",
        "i2ywrmsMP_EF",
        "Blrrs1sWwkSx",
        "Kx-dZSFJz9cK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "master-ai-tec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
