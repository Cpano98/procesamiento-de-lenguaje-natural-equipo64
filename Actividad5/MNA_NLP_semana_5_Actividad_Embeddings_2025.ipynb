{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Adtividad Semana 5\n",
        "\n",
        "### **Vectores Embebidos de OpenAI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   Elemento de lista\n",
        "*   Carlos Pano Hernandez - A01066264\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "wCL2p6MA8NuT"
      },
      "outputs": [],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "#import spacy\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_lg\n",
        "#import en_core_web_lg\n",
        "#from scipy.spatial import distance\n",
        "\n",
        "\n",
        "#import fasttext\n",
        "#import fasttext.util\n",
        "\n",
        "import openai\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "#from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stop words excluding negation words:\n",
            "----------------------------------------\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'd', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/charliepano/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/charliepano/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Descargamos los paquetes necesarios\n",
        "nltk.download('punkt')   \n",
        "nltk.download('stopwords')\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "mystopwords = set(stopwords.words('english')) - set(negwords)\n",
        "print(\"\\nStop words excluding negation words:\")\n",
        "print(\"-\" * 40)\n",
        "print(sorted(mystopwords))\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the English fasttext model\n",
        "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "#fasttext.util.download_model('es', if_exists='ignore')  # Español\n",
        "#cc_en = fasttext.load_model('cc.en.300.bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7wc107K-oIV4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gut.\n"
          ]
        }
      ],
      "source": [
        "# Incluye las celdas necesarias para tu acceso a la API de OpenAI.\n",
        "\n",
        "# Load environment variables from the specific .env file -> Amigos, aqui cada quien configuere su propia API key en el .env\n",
        "# El .gitignore ya excluye el mismo.\n",
        "\n",
        "load_dotenv('.env')\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Prueba de la API de OpenAI\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Hola, como estas? (Responde en 1 palabra y en Aleman)\"\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "# **Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
        "\n",
        "**Llama simplemente \"df\" a dicho DataFrame.**\n",
        "\n",
        "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "T_lyEFRkxzC6"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Reding the three text files\n",
        "amazon_df = pd.read_csv('Data/amazon5.txt', sep='\\t', header=None, names=['text', 'label'])\n",
        "imdb_df = pd.read_csv('Data/imdb5.txt', sep='\\t', header=None, names=['text', 'label']) \n",
        "yelp_df = pd.read_csv('Data/yelp5.txt', sep='\\t', header=None, names=['text', 'label'])\n",
        "\n",
        "# Add source column to identify which dataset each review came from\n",
        "amazon_df['source'] = 'amazon'\n",
        "imdb_df['source'] = 'imdb'\n",
        "yelp_df['source'] = 'yelp'\n",
        "\n",
        "# Concatenate\n",
        "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "3-w1xMLYnm9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   text    3000 non-null   object \n",
            " 1   label   2000 non-null   float64\n",
            " 2   source  3000 non-null   object \n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 70.4+ KB\n"
          ]
        }
      ],
      "source": [
        "# Verifiquemos la información del DataFrame:\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "NfVUcYe1nubT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  label  source\n",
            "0  So there is no way for me to plug it in here i...    0.0  amazon\n",
            "1                        Good case, Excellent value.    1.0  amazon\n",
            "2                             Great for the jawbone.    1.0  amazon\n",
            "3  Tied to charger for conversations lasting more...    0.0  amazon\n",
            "4                                  The mic is great.    1.0  amazon\n"
          ]
        }
      ],
      "source": [
        "# Y veamos sus primeros registros:\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "X = df.text   \n",
        "Y = df.label    \n",
        "\n",
        "# Verificamos que las dimensiones sean las esperadas (3,000 reviews)\n",
        "assert X.shape == (3000,)          \n",
        "assert Y.shape == (3000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "# **Pregunta - 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6JF5BommZ6"
      },
      "source": [
        "Realiza el proceso de limpieza. Aplica el preprocesamiento que consideres adecuado.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "TsnvMp-7oYCM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens after cleaning:\n",
            "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
            "['mic', 'great']\n",
            "\n",
            "Tokens after normalization:\n",
            "['way', 'plug', 'u', 'unless', 'go', 'convert']\n",
            "['good', 'case', 'excel', 'valu']\n",
            "['great', 'jawbon']\n",
            "['tie', 'charger', 'convers', 'last', 'minut', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing functions\n",
        "\n",
        "def clean_tok(doc):\n",
        "    # Basic cleaning and tokenization\n",
        "    doc = doc.lower()\n",
        "    \n",
        "    # Remove non-letters\n",
        "    doc = re.sub(r'[^a-z\\s]', ' ', doc)\n",
        "    \n",
        "    # Split into words\n",
        "    tokens = doc.split()\n",
        "    \n",
        "    # Filter stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Remove single chars\n",
        "    tokens = [token for token in tokens if len(token) > 1]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # Token normalization\n",
        "    ps = PorterStemmer()\n",
        "    wnl = WordNetLemmatizer()\n",
        "    \n",
        "    tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        # Clean repeating chars\n",
        "        token = re.sub(r'(.)\\1{2,}', r'\\1\\1', token)\n",
        "        token = ps.stem(token)\n",
        "        token = wnl.lemmatize(token)\n",
        "        tokens.append(token)\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "# Clean and tokenize\n",
        "Xclean = [clean_tok(x) for x in X]   \n",
        "print(\"Tokens after cleaning:\")\n",
        "print(*Xclean[0:5], sep='\\n')\n",
        "\n",
        "# Normalize tokens\n",
        "Xclean = [clean_doc(x) for x in Xclean]  \n",
        "print(\"\\nTokens after normalization:\")\n",
        "print(*Xclean[0:5], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "7jlQuoI2o33T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['way', 'plug', 'u', 'unless', 'go', 'convert']\n",
            "['good', 'case', 'excel', 'valu']\n",
            "['great', 'jawbon']\n",
            "['tie', 'charger', 'convers', 'last', 'minut', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "# Despleguemos los primeros comentarios después de tu proceso de limpieza:\n",
        "\n",
        "for x in Xclean[0:5]:\n",
        "  print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygchEdcKqIzU"
      },
      "source": [
        "# **Pregunta - 3:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEIOkkl9Dot"
      },
      "source": [
        "\n",
        "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "b0SAcYdq9X0w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ************* Inicia la sección de agregar código:*****************************\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1) \n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "\n",
        "# *********** Termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# verificemos las dimensiones obtenidas:\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "# **Pregunta - 4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jENsKiN99r3F"
      },
      "source": [
        "\n",
        "\n",
        "Construye tu vocabulario a continuación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "TzJntmLPqPqC"
      },
      "outputs": [],
      "source": [
        "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
        "#     con un tamaño que consideres adecuado:\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "midiccionario = Counter()    \n",
        "\n",
        "for k in range(len(x_train)):\n",
        "  midiccionario.update(x_train[k])\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yTDZ0Rr86CUP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del diccionario: 3145\n",
            "\n",
            "(word,frequency):\n",
            "[('good', 164), ('movi', 140), ('great', 139), ('phone', 134), ('film', 130), ('work', 113), ('time', 102), ('like', 101), ('one', 100), ('place', 87)]\n"
          ]
        }
      ],
      "source": [
        "# b.\tIndica el tamaño del vocabulario generado.\n",
        "\n",
        "# ******* Inicia la sección de agregar código: ***********\n",
        "\n",
        "\n",
        "print('Longitud del diccionario:', len(midiccionario))  \n",
        "print('\\n(word,frequency):') \n",
        "print(midiccionario.most_common(10)) \n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDa4EhTqrw15"
      },
      "source": [
        "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "Existen varias razones, entre las principales se encuentran:\n",
        "\n",
        "1. **Evitar data leakage:** Si usáramos otros conjuntos para nuestro diccionario, estaríamos filtrando información que debería ser completamente desconocida durante el entrenamiento.\n",
        "2. **Simular escenarios reales:** En un caso real, no tendremos acceso a datos futuros.\n",
        "3. **Detectar overfitting:** Si el vocabulario se construyera con todos los datos, sería más difícil detectar si el modelo está memorizando en lugar de aprendiendo patrones generalizables.\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7ykjxQI3rpxx"
      },
      "outputs": [],
      "source": [
        "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
        "#     validación y prueba para que todos los comentarios usen solamente las\n",
        "#     palabras de este vocabulario.\n",
        "\n",
        "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Crear conjuntos filtrados usando solo palabras del vocabulario\n",
        "train_x = []\n",
        "val_x = []\n",
        "test_x = []\n",
        "\n",
        "# Filtrar conjunto de entrenamiento\n",
        "for comment in x_train:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    train_x.append(filtered_comment)\n",
        "\n",
        "# Filtrar conjunto de validación  \n",
        "for comment in x_val:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    val_x.append(filtered_comment)\n",
        "\n",
        "# Filtrar conjunto de prueba\n",
        "for comment in x_test:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    test_x.append(filtered_comment)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "iYF2RGuPtQTC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['co', 'star', 'fare', 'much', 'better', 'peopl', 'like', 'morgan', 'freeman', 'jonah', 'hill', 'ed', 'helm', 'wast']\n",
            "['tonight', 'elk', 'filet', 'special', 'suck']\n",
            "['paid', 'bill', 'tip', 'felt', 'server', 'terribl', 'job']\n",
            "['call', 'steakhous', 'properli', 'cook', 'steak', 'understand']\n",
            "['howev', 'keypad', 'tinni', 'sometim', 'reach', 'wrong', 'button']\n"
          ]
        }
      ],
      "source": [
        "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
        "\n",
        "for ss in train_x[0:5]:\n",
        "  print(ss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "# **Pregunta - 5:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnHHAza5_P5Z"
      },
      "source": [
        "\n",
        "#### **Incluye aquí un resumen de las características y diferencias que tiene al menos los tres modelos de OpenAI indicados: \"text-embedding-3-small\", \"text-embedding-3-large\" y \"text-embedding-ada-002\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTI9xSgF_Xc8"
      },
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqRl7fT_fn2"
      },
      "source": [
        "# **Pregunta - 6:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKqQk03oqoOD"
      },
      "source": [
        "#### **Diccionario clave-valor de palabras del diccionario y vectores embebidos.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "UdK-jMfLxHLY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo vocabulary_embeddings.pkl ya existe. Omitiendo la generación de embeddings con modelo: text-embedding-3-small.\n",
            "\n",
            "Ejemplo de embedding para la palabra 'co':\n",
            "Dimensiones del vector: 1536\n",
            "Primeros 5 valores del vector: [0.04341873, 0.0015039851, 0.0046208845, 0.042929508, 0.029063033]\n"
          ]
        }
      ],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Seleccionamos el modelo de embeddings (usando el más económico de los mencionados)\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_ENCODING = \"cl100k_base\"  # Este es el encoding recomendado para el modelo\n",
        "EMBEDDING_DIMENSIONS = 1536  # Dimensiones del modelo small\n",
        "\n",
        "# Función con reintentos para manejar errores de API\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
        "def get_embedding(text):\n",
        "    response = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=text,\n",
        "        encoding_format=\"float\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Verificamos si el archivo ya existe\n",
        "if not os.path.exists('vocabulary_embeddings.pkl'):\n",
        "    embeddings_dict = {}\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Generamos embeddings para cada palabra en el vocabulario\n",
        "    print(\"Generando embeddings para el vocabulario...\")\n",
        "    for word in tqdm(midiccionario):\n",
        "        embedding = get_embedding(word)\n",
        "        embeddings_dict[word] = embedding\n",
        "        # Aproximadamente 1 token por palabra para palabras simples\n",
        "        total_tokens += 1\n",
        "\n",
        "    print(f\"\\nTotal de tokens utilizados: {total_tokens}\")\n",
        "    print(f\"Dimensiones del vector de embeddings: {EMBEDDING_DIMENSIONS}\")\n",
        "    print(f\"Tamaño del vocabulario procesado: {len(embeddings_dict)}\")\n",
        "\n",
        "    # Guardamos el diccionario de embeddings\n",
        "    with open('vocabulary_embeddings.pkl', 'wb') as f:\n",
        "        pickle.dump(embeddings_dict, f)\n",
        "\n",
        "    print(\"\\nDiccionario de embeddings guardado en 'vocabulary_embeddings.pkl'\")\n",
        "\n",
        "    # Mostramos un ejemplo del diccionario\n",
        "    sample_word = list(embeddings_dict.keys())[0]\n",
        "    print(f\"\\nEjemplo de embedding para la palabra '{sample_word}':\")\n",
        "    print(f\"Dimensiones del vector: {len(embeddings_dict[sample_word])}\")\n",
        "    print(f\"Primeros 5 valores del vector: {embeddings_dict[sample_word][:5]}\")\n",
        "else:\n",
        "    # Cargamos el diccionario de embeddings existente\n",
        "    with open('vocabulary_embeddings.pkl', 'rb') as f:\n",
        "        embeddings_dict = pickle.load(f)\n",
        "        \n",
        "    print(f\"El archivo vocabulary_embeddings.pkl ya existe. Omitiendo la generación de embeddings con modelo: {EMBEDDING_MODEL}.\")\n",
        "    \n",
        "    # Mostramos un ejemplo del diccionario\n",
        "    sample_word = list(embeddings_dict.keys())[0]\n",
        "    print(f\"\\nEjemplo de embedding para la palabra '{sample_word}':\")\n",
        "    print(f\"Dimensiones del vector: {len(embeddings_dict[sample_word])}\")\n",
        "    print(f\"Primeros 5 valores del vector: {embeddings_dict[sample_word][:5]}\")\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4S7q0yR0Mpi"
      },
      "source": [
        "# **Pregunta - 7:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyeOrkoaC1eq"
      },
      "source": [
        "\n",
        "\n",
        "Generamos los vectores embebidos a partir de los conjuntos de entrenamiento, validación y prueba.\n",
        "\n",
        "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "wnfQpkxg0Usq"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Función para obtener el vector promedio de un comentario\n",
        "def get_avg_embedding(text, embeddings_dict, embedding_dim=300):\n",
        "    # Si el texto es una lista, lo unimos en un string\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)\n",
        "    \n",
        "    # Tokenizamos el texto y nos aseguramos que las palabras estén en el diccionario\n",
        "    tokens = [token for token in text.split() if token in embeddings_dict]\n",
        "    \n",
        "    if not tokens:  # Si no hay tokens válidos, retornamos vector de ceros\n",
        "        return np.zeros(embedding_dim)\n",
        "    \n",
        "    # Obtenemos los embeddings y verificamos que tengan la dimensión correcta\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        embedding = embeddings_dict[token]\n",
        "        if len(embedding) == embedding_dim:  # Solo usamos embeddings de la dimensión correcta\n",
        "            embeddings.append(embedding)\n",
        "    \n",
        "    if not embeddings:  # Si no hay embeddings válidos\n",
        "        return np.zeros(embedding_dim)\n",
        "    \n",
        "    # Convertimos a array y calculamos el promedio\n",
        "    embeddings = np.array(embeddings)\n",
        "    return np.mean(embeddings, axis=0)\n",
        "\n",
        "# Generamos los embeddings para cada conjunto\n",
        "trainEmb = np.array([get_avg_embedding(text, embeddings_dict) for text in train_x])\n",
        "valEmb = np.array([get_avg_embedding(text, embeddings_dict) for text in val_x]) \n",
        "testEmb = np.array([get_avg_embedding(text, embeddings_dict) for text in test_x])\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "J3BBF96D0N8Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Emb: (2100, 300)\n",
            "Val-Emb: (450, 300)\n",
            "Test-Emb: (450, 300)\n"
          ]
        }
      ],
      "source": [
        "# Veamos las dimensiones de cada conjunto embebido:\n",
        "\n",
        "print(\"Train-Emb:\", trainEmb.shape)\n",
        "print(\"Val-Emb:\", valEmb.shape)\n",
        "print(\"Test-Emb:\", testEmb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibp1LA91CP_"
      },
      "source": [
        "# **Pregunta - 8:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxC9K0VnGOwG"
      },
      "source": [
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
        "\n",
        "Compara los resultados con los de la semana anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycwjD8ztGOL7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[96], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# REGRESIÓN LOGÍSTICA:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo de regresión logística\u001b[39;00m\n\u001b[1;32m      6\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(trainEmb, y_train)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1224\u001b[0m     X,\n\u001b[1;32m   1225\u001b[0m     y,\n\u001b[1;32m   1226\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[1;32m   1228\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1230\u001b[0m )\n\u001b[1;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:1340\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1339\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1340\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1341\u001b[0m     _ensure_no_complex_data(y)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "# REGRESIÓN LOGÍSTICA:\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Entrenamos el modelo de regresión logística\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(trainEmb, y_train)\n",
        "\n",
        "# Hacemos predicciones en conjunto de entrenamiento y validación\n",
        "train_pred_lr = lr_model.predict(trainEmb)\n",
        "val_pred_lr = lr_model.predict(valEmb)\n",
        "\n",
        "# Calculamos exactitud (accuracy)\n",
        "print(\"Regresión Logística:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Exactitud en entrenamiento: {accuracy_score(y_train, train_pred_lr):.4f}\")\n",
        "print(f\"Exactitud en validación: {accuracy_score(y_val, val_pred_lr):.4f}\")\n",
        "\n",
        "# Mostramos reporte detallado de métricas\n",
        "print(\"\\nReporte detallado en conjunto de entrenamiento:\")\n",
        "print(classification_report(y_train, train_pred_lr))\n",
        "print(\"\\nReporte detallado en conjunto de validación:\")\n",
        "print(classification_report(y_val, val_pred_lr))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "N4n70GHW0sl3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[93], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# BOSQUE ALEATORIO (Random Forest):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo de Random Forest\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(trainEmb, y_train)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Hacemos predicciones en conjunto de entrenamiento y validación\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_pred_rf \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(trainEmb)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    364\u001b[0m     X,\n\u001b[1;32m    365\u001b[0m     y,\n\u001b[1;32m    366\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    368\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[1;32m    369\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:1328\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1328\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1329\u001b[0m         y,\n\u001b[1;32m   1330\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1331\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1332\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1335\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1336\u001b[0m     )\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/master-ai-tec/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "# BOSQUE ALEATORIO (Random Forest):\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Entrenamos el modelo de Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(trainEmb, y_train)\n",
        "\n",
        "# Hacemos predicciones en conjunto de entrenamiento y validación\n",
        "train_pred_rf = rf_model.predict(trainEmb)\n",
        "val_pred_rf = rf_model.predict(valEmb)\n",
        "\n",
        "# Calculamos exactitud (accuracy)\n",
        "print(\"Random Forest:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Exactitud en entrenamiento: {accuracy_score(y_train, train_pred_rf):.4f}\")\n",
        "print(f\"Exactitud en validación: {accuracy_score(y_val, val_pred_rf):.4f}\")\n",
        "\n",
        "# Mostramos reporte detallado de métricas\n",
        "print(\"\\nReporte detallado en conjunto de entrenamiento:\")\n",
        "print(classification_report(y_train, train_pred_rf))\n",
        "print(\"\\nReporte detallado en conjunto de validación:\")\n",
        "print(classification_report(y_val, val_pred_rf))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDIiSHvg0_hm"
      },
      "source": [
        "# **Pregunta - 9:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJtALGZHrGk"
      },
      "source": [
        "\n",
        "\n",
        "Reporte del mejor modelo con el conjunto de Prueba (Test).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETv4VLjP1GYt"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbhBUBKJp1MB"
      },
      "source": [
        "# **Pregunta - 10:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zPSi-H7p6ga"
      },
      "outputs": [],
      "source": [
        "# Incluye todas las líneas de código y celdas que consideres adecuadas para este ejercicio.\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkh2WfN1MC1"
      },
      "source": [
        "# **Pregunta - 11:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySFuDQtVuK5"
      },
      "source": [
        "\n",
        "\n",
        "Incluye tus comentarios finales de la actividad.\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKHmQTbWJT1"
      },
      "source": [
        "# **Fin de la Actividad de Vectores Embebidos - OpenAI**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4c34ZOnna3Gu",
        "MfZZ0stLmWJN",
        "ygchEdcKqIzU",
        "1qjKoEqiqBN1",
        "RS0Hxj25vTWh",
        "ToqRl7fT_fn2",
        "W4S7q0yR0Mpi",
        "pibp1LA91CP_",
        "WDIiSHvg0_hm",
        "NbhBUBKJp1MB",
        "YCkh2WfN1MC1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "master-ai-tec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
