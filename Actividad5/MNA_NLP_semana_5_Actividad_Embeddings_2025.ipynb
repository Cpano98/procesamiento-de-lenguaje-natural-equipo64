{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Adtividad Semana 5\n",
        "\n",
        "### **Vectores Embebidos de OpenAI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   Elemento de lista\n",
        "*   Carlos Pano Hernandez - A01066264\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wCL2p6MA8NuT"
      },
      "outputs": [],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "#import spacy\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_lg\n",
        "#import en_core_web_lg\n",
        "#from scipy.spatial import distance\n",
        "\n",
        "\n",
        "#import fasttext\n",
        "#import fasttext.util\n",
        "\n",
        "import openai\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "#from tenacity import retry, wait_random_exponential, stop_after_attempt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stop words excluding negation words:\n",
            "----------------------------------------\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'd', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/charliepano/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/charliepano/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Descargamos los paquetes necesarios\n",
        "nltk.download('punkt')   \n",
        "nltk.download('stopwords')\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "mystopwords = set(stopwords.words('english')) - set(negwords)\n",
        "print(\"\\nStop words excluding negation words:\")\n",
        "print(\"-\" * 40)\n",
        "print(sorted(mystopwords))\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the English fasttext model\n",
        "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "#fasttext.util.download_model('es', if_exists='ignore')  # Español\n",
        "#cc_en = fasttext.load_model('cc.en.300.bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7wc107K-oIV4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gut.\n"
          ]
        }
      ],
      "source": [
        "# Incluye las celdas necesarias para tu acceso a la API de OpenAI.\n",
        "\n",
        "# Load environment variables from the specific .env file -> Amigos, aqui cada quien configuere su propia API key en el .env\n",
        "# El .gitignore ya excluye el mismo.\n",
        "\n",
        "load_dotenv('.env')\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Prueba de la API de OpenAI\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Hola, como estas? (Responde en 1 palabra y en Aleman)\"\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "# **Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
        "\n",
        "**Llama simplemente \"df\" a dicho DataFrame.**\n",
        "\n",
        "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "T_lyEFRkxzC6"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Reding the three text files\n",
        "amazon_df = pd.read_csv('Data/amazon5.txt', sep='\\t', header=None, names=['text', 'label'])\n",
        "imdb_df = pd.read_csv('Data/imdb5.txt', sep='\\t', header=None, names=['text', 'label']) \n",
        "yelp_df = pd.read_csv('Data/yelp5.txt', sep='\\t', header=None, names=['text', 'label'])\n",
        "\n",
        "# Add source column to identify which dataset each review came from\n",
        "amazon_df['source'] = 'amazon'\n",
        "imdb_df['source'] = 'imdb'\n",
        "yelp_df['source'] = 'yelp'\n",
        "\n",
        "# Concatenate\n",
        "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3-w1xMLYnm9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   text    3000 non-null   object \n",
            " 1   label   2000 non-null   float64\n",
            " 2   source  3000 non-null   object \n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 70.4+ KB\n"
          ]
        }
      ],
      "source": [
        "# Verifiquemos la información del DataFrame:\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NfVUcYe1nubT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  label  source\n",
            "0  So there is no way for me to plug it in here i...    0.0  amazon\n",
            "1                        Good case, Excellent value.    1.0  amazon\n",
            "2                             Great for the jawbone.    1.0  amazon\n",
            "3  Tied to charger for conversations lasting more...    0.0  amazon\n",
            "4                                  The mic is great.    1.0  amazon\n"
          ]
        }
      ],
      "source": [
        "# Y veamos sus primeros registros:\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "X = df.text   \n",
        "Y = df.label    \n",
        "\n",
        "# Verificamos que las dimensiones sean las esperadas (3,000 reviews)\n",
        "assert X.shape == (3000,)          \n",
        "assert Y.shape == (3000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "# **Pregunta - 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6JF5BommZ6"
      },
      "source": [
        "Realiza el proceso de limpieza. Aplica el preprocesamiento que consideres adecuado.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TsnvMp-7oYCM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens after cleaning:\n",
            "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
            "['mic', 'great']\n",
            "\n",
            "Tokens after normalization:\n",
            "['way', 'plug', 'u', 'unless', 'go', 'convert']\n",
            "['good', 'case', 'excel', 'valu']\n",
            "['great', 'jawbon']\n",
            "['tie', 'charger', 'convers', 'last', 'minut', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing functions\n",
        "\n",
        "def clean_tok(doc):\n",
        "    # Basic cleaning and tokenization\n",
        "    doc = doc.lower()\n",
        "    \n",
        "    # Remove non-letters\n",
        "    doc = re.sub(r'[^a-z\\s]', ' ', doc)\n",
        "    \n",
        "    # Split into words\n",
        "    tokens = doc.split()\n",
        "    \n",
        "    # Filter stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Remove single chars\n",
        "    tokens = [token for token in tokens if len(token) > 1]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # Token normalization\n",
        "    ps = PorterStemmer()\n",
        "    wnl = WordNetLemmatizer()\n",
        "    \n",
        "    tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        # Clean repeating chars\n",
        "        token = re.sub(r'(.)\\1{2,}', r'\\1\\1', token)\n",
        "        token = ps.stem(token)\n",
        "        token = wnl.lemmatize(token)\n",
        "        tokens.append(token)\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "# Clean and tokenize\n",
        "Xclean = [clean_tok(x) for x in X]   \n",
        "print(\"Tokens after cleaning:\")\n",
        "print(*Xclean[0:5], sep='\\n')\n",
        "\n",
        "# Normalize tokens\n",
        "Xclean = [clean_doc(x) for x in Xclean]  \n",
        "print(\"\\nTokens after normalization:\")\n",
        "print(*Xclean[0:5], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7jlQuoI2o33T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['way', 'plug', 'u', 'unless', 'go', 'convert']\n",
            "['good', 'case', 'excel', 'valu']\n",
            "['great', 'jawbon']\n",
            "['tie', 'charger', 'convers', 'last', 'minut', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "# Despleguemos los primeros comentarios después de tu proceso de limpieza:\n",
        "\n",
        "for x in Xclean[0:5]:\n",
        "  print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygchEdcKqIzU"
      },
      "source": [
        "# **Pregunta - 3:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEIOkkl9Dot"
      },
      "source": [
        "\n",
        "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "b0SAcYdq9X0w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ************* Inicia la sección de agregar código:*****************************\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1) \n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "\n",
        "# *********** Termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# verificemos las dimensiones obtenidas:\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "# **Pregunta - 4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jENsKiN99r3F"
      },
      "source": [
        "\n",
        "\n",
        "Construye tu vocabulario a continuación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TzJntmLPqPqC"
      },
      "outputs": [],
      "source": [
        "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
        "#     con un tamaño que consideres adecuado:\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "midiccionario = Counter()    \n",
        "\n",
        "for k in range(len(x_train)):\n",
        "  midiccionario.update(x_train[k])\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yTDZ0Rr86CUP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del diccionario: 3145\n",
            "\n",
            "(word,frequency):\n",
            "[('good', 164), ('movi', 140), ('great', 139), ('phone', 134), ('film', 130), ('work', 113), ('time', 102), ('like', 101), ('one', 100), ('place', 87)]\n"
          ]
        }
      ],
      "source": [
        "# b.\tIndica el tamaño del vocabulario generado.\n",
        "\n",
        "# ******* Inicia la sección de agregar código: ***********\n",
        "\n",
        "\n",
        "print('Longitud del diccionario:', len(midiccionario))  \n",
        "print('\\n(word,frequency):') \n",
        "print(midiccionario.most_common(10)) \n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDa4EhTqrw15"
      },
      "source": [
        "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "Existen varias razones, entre las principales se encuentran:\n",
        "\n",
        "1. **Evitar data leakage:** Si usáramos otros conjuntos para nuestro diccionario, estaríamos filtrando información que debería ser completamente desconocida durante el entrenamiento.\n",
        "2. **Simular escenarios reales:** En un caso real, no tendremos acceso a datos futuros.\n",
        "3. **Detectar overfitting:** Si el vocabulario se construyera con todos los datos, sería más difícil detectar si el modelo está memorizando en lugar de aprendiendo patrones generalizables.\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7ykjxQI3rpxx"
      },
      "outputs": [],
      "source": [
        "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
        "#     validación y prueba para que todos los comentarios usen solamente las\n",
        "#     palabras de este vocabulario.\n",
        "\n",
        "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "# Crear conjuntos filtrados usando solo palabras del vocabulario\n",
        "train_x = []\n",
        "val_x = []\n",
        "test_x = []\n",
        "\n",
        "# Filtrar conjunto de entrenamiento\n",
        "for comment in x_train:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    train_x.append(filtered_comment)\n",
        "\n",
        "# Filtrar conjunto de validación  \n",
        "for comment in x_val:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    val_x.append(filtered_comment)\n",
        "\n",
        "# Filtrar conjunto de prueba\n",
        "for comment in x_test:\n",
        "    filtered_comment = [word for word in comment if word in midiccionario]\n",
        "    test_x.append(filtered_comment)\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iYF2RGuPtQTC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['co', 'star', 'fare', 'much', 'better', 'peopl', 'like', 'morgan', 'freeman', 'jonah', 'hill', 'ed', 'helm', 'wast']\n",
            "['tonight', 'elk', 'filet', 'special', 'suck']\n",
            "['paid', 'bill', 'tip', 'felt', 'server', 'terribl', 'job']\n",
            "['call', 'steakhous', 'properli', 'cook', 'steak', 'understand']\n",
            "['howev', 'keypad', 'tinni', 'sometim', 'reach', 'wrong', 'button']\n"
          ]
        }
      ],
      "source": [
        "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
        "\n",
        "for ss in train_x[0:5]:\n",
        "  print(ss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "# **Pregunta - 5:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnHHAza5_P5Z"
      },
      "source": [
        "\n",
        "#### **Incluye aquí un resumen de las características y diferencias que tiene al menos los tres modelos de OpenAI indicados: \"text-embedding-3-small\", \"text-embedding-3-large\" y \"text-embedding-ada-002\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTI9xSgF_Xc8"
      },
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqRl7fT_fn2"
      },
      "source": [
        "# **Pregunta - 6:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKqQk03oqoOD"
      },
      "source": [
        "#### **Diccionario clave-valor de palabras del diccionario y vectores embebidos.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdK-jMfLxHLY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo vocabulary_embeddings.pkl ya existe. Omitiendo la generación de embeddings.\n",
            "\n",
            "Ejemplo de embedding para la palabra 'co':\n",
            "Dimensiones del vector: 1536\n",
            "Primeros 5 valores del vector: [0.04341873, 0.0015039851, 0.0046208845, 0.042929508, 0.029063033]\n"
          ]
        }
      ],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Seleccionamos el modelo de embeddings (usando el más económico de los mencionados)\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_ENCODING = \"cl100k_base\"  # Este es el encoding recomendado para el modelo\n",
        "EMBEDDING_DIMENSIONS = 1536  # Dimensiones del modelo small\n",
        "\n",
        "# Función con reintentos para manejar errores de API\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
        "def get_embedding(text):\n",
        "    response = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=text,\n",
        "        encoding_format=\"float\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Verificamos si el archivo ya existe\n",
        "if not os.path.exists('vocabulary_embeddings.pkl'):\n",
        "    embeddings_dict = {}\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Generamos embeddings para cada palabra en el vocabulario\n",
        "    print(\"Generando embeddings para el vocabulario...\")\n",
        "    for word in tqdm(midiccionario):\n",
        "        embedding = get_embedding(word)\n",
        "        embeddings_dict[word] = embedding\n",
        "        # Aproximadamente 1 token por palabra para palabras simples\n",
        "        total_tokens += 1\n",
        "\n",
        "    print(f\"\\nTotal de tokens utilizados: {total_tokens}\")\n",
        "    print(f\"Dimensiones del vector de embeddings: {EMBEDDING_DIMENSIONS}\")\n",
        "    print(f\"Tamaño del vocabulario procesado: {len(embeddings_dict)}\")\n",
        "\n",
        "    # Guardamos el diccionario de embeddings\n",
        "    with open('vocabulary_embeddings.pkl', 'wb') as f:\n",
        "        pickle.dump(embeddings_dict, f)\n",
        "\n",
        "    print(\"\\nDiccionario de embeddings guardado en 'vocabulary_embeddings.pkl'\")\n",
        "\n",
        "    # Mostramos un ejemplo del diccionario\n",
        "    sample_word = list(embeddings_dict.keys())[0]\n",
        "    print(f\"\\nEjemplo de embedding para la palabra '{sample_word}':\")\n",
        "    print(f\"Dimensiones del vector: {len(embeddings_dict[sample_word])}\")\n",
        "    print(f\"Primeros 5 valores del vector: {embeddings_dict[sample_word][:5]}\")\n",
        "else:\n",
        "    # Cargamos el diccionario de embeddings existente\n",
        "    with open('vocabulary_embeddings.pkl', 'rb') as f:\n",
        "        embeddings_dict = pickle.load(f)\n",
        "        \n",
        "    print(\"El archivo vocabulary_embeddings.pkl ya existe. Omitiendo la generación de embeddings.\")\n",
        "    \n",
        "    # Mostramos un ejemplo del diccionario\n",
        "    sample_word = list(embeddings_dict.keys())[0]\n",
        "    print(f\"\\nEjemplo de embedding para la palabra '{sample_word}':\")\n",
        "    print(f\"Dimensiones del vector: {len(embeddings_dict[sample_word])}\")\n",
        "    print(f\"Primeros 5 valores del vector: {embeddings_dict[sample_word][:5]}\")\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4S7q0yR0Mpi"
      },
      "source": [
        "# **Pregunta - 7:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyeOrkoaC1eq"
      },
      "source": [
        "\n",
        "\n",
        "Generamos los vectores embebidos a partir de los conjuntos de entrenamiento, validación y prueba.\n",
        "\n",
        "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnfQpkxg0Usq"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3BBF96D0N8Z"
      },
      "outputs": [],
      "source": [
        "# Veamos las dimensiones de cada conjunto embebido:\n",
        "\n",
        "print(\"Train-Emb:\", trainEmb.shape)\n",
        "print(\"Val-Emb:\", valEmb.shape)\n",
        "print(\"Test-Emb:\", testEmb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibp1LA91CP_"
      },
      "source": [
        "# **Pregunta - 8:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxC9K0VnGOwG"
      },
      "source": [
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
        "\n",
        "Compara los resultados con los de la semana anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycwjD8ztGOL7"
      },
      "outputs": [],
      "source": [
        "# REGRESIÓN LOGÍSTICA:\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4n70GHW0sl3"
      },
      "outputs": [],
      "source": [
        "# BOSQUE ALEATORIO (Random Forest):\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDIiSHvg0_hm"
      },
      "source": [
        "# **Pregunta - 9:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJtALGZHrGk"
      },
      "source": [
        "\n",
        "\n",
        "Reporte del mejor modelo con el conjunto de Prueba (Test).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETv4VLjP1GYt"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbhBUBKJp1MB"
      },
      "source": [
        "# **Pregunta - 10:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zPSi-H7p6ga"
      },
      "outputs": [],
      "source": [
        "# Incluye todas las líneas de código y celdas que consideres adecuadas para este ejercicio.\n",
        "\n",
        "\n",
        "None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkh2WfN1MC1"
      },
      "source": [
        "# **Pregunta - 11:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySFuDQtVuK5"
      },
      "source": [
        "\n",
        "\n",
        "Incluye tus comentarios finales de la actividad.\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKHmQTbWJT1"
      },
      "source": [
        "# **Fin de la Actividad de Vectores Embebidos - OpenAI**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4c34ZOnna3Gu",
        "MfZZ0stLmWJN",
        "ygchEdcKqIzU",
        "1qjKoEqiqBN1",
        "RS0Hxj25vTWh",
        "ToqRl7fT_fn2",
        "W4S7q0yR0Mpi",
        "pibp1LA91CP_",
        "WDIiSHvg0_hm",
        "NbhBUBKJp1MB",
        "YCkh2WfN1MC1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "master-ai-tec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
